{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used Car Regression\n",
    "**ML Project 1 - Spring 2022**\n",
    "\n",
    "Collaborators: Ben DeSollar and Matt McDonell\n",
    "\n",
    "Through research we found that a called GradientBoostingRegressor worked the best after testing a couple different model including\n",
    "    KNeighborsRegressor, Lasso, Ridge, and DecisionTreeRegressor. We improved our results by modifying our feature selection to include as many features as we \n",
    "    could to help predict the best values. Things like weight and Deaft year did not seem to be as important as the draft round and PIE. By modifying our featuiues \n",
    "    during testing we were able to find the best model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "Import the required packages and frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from mlwpy import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import (datasets, neighbors,\n",
    "                     naive_bayes,\n",
    "                     model_selection as skms,\n",
    "                     linear_model, dummy,\n",
    "                     metrics,\n",
    "                     pipeline,\n",
    "                     preprocessing as skpre) \n",
    "import csv\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Read in the .csv files and create DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_df = pd.read_csv(\"cars_reg_train.csv\") \n",
    "data_test_df = pd.read_csv(\"cars_reg_test.csv\")\n",
    "data_train_ft = data_train_df.drop('price', axis=1)\n",
    "data_train_tgt = data_train_df[\"price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "Focus down to the features we think will have the largest impact on points scored. \n",
    "\n",
    "We tried to stick to more of the physical attributes like age or height over subjective factors, like city or roster status. We believe points are more about the players themselves than the citys/schools they play for.\n",
    "As well as including whether they are in the NBA or not, active, and the draft selection. Lastly PIE seems to have the greatest impact, even though there are not a lot of them in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0        int64\n",
      "url              object\n",
      "region           object\n",
      "region_url       object\n",
      "year            float64\n",
      "manufacturer     object\n",
      "model            object\n",
      "condition        object\n",
      "cylinders        object\n",
      "fuel             object\n",
      "odometer        float64\n",
      "title_status     object\n",
      "transmission     object\n",
      "VIN              object\n",
      "drive            object\n",
      "size             object\n",
      "type             object\n",
      "paint_color      object\n",
      "image_url        object\n",
      "description      object\n",
      "state            object\n",
      "lat             float64\n",
      "long            float64\n",
      "posting_date     object\n",
      "dtype: object\n",
      "   Unnamed: 0                                                url  \\\n",
      "0      259073  https://newjersey.craigslist.org/cto/d/bloomin...   \n",
      "1      445077  https://denver.craigslist.org/cto/d/henderson-...   \n",
      "2      109862  https://spacecoast.craigslist.org/cto/d/merrit...   \n",
      "3       17375  https://yuma.craigslist.org/cto/d/yuma-2016-fo...   \n",
      "4       51906  https://sacramento.craigslist.org/ctd/d/san-ra...   \n",
      "\n",
      "         region                         region_url                 year  \\\n",
      "0  north jersey   https://newjersey.craigslist.org           2,010.0000   \n",
      "1        denver      https://denver.craigslist.org           1,967.0000   \n",
      "2   space coast  https://spacecoast.craigslist.org           1,999.0000   \n",
      "3          yuma        https://yuma.craigslist.org           2,016.0000   \n",
      "4    sacramento  https://sacramento.craigslist.org           2,017.0000   \n",
      "\n",
      "  manufacturer                 model  condition    cylinders fuel  ...  drive  \\\n",
      "0         audi  a5 2.0t premium plus  excellent  4 cylinders  gas  ...    4wd   \n",
      "1    chevrolet                camaro   like new  8 cylinders  gas  ...    rwd   \n",
      "2          gmc          yukon denali  excellent  8 cylinders  gas  ...    4wd   \n",
      "3         ford    fusion se ecoboost  excellent  4 cylinders  gas  ...    fwd   \n",
      "4        lexus             es es 350  excellent  6 cylinders  gas  ...    NaN   \n",
      "\n",
      "        size         type paint_color  \\\n",
      "0   mid-size  convertible       white   \n",
      "1   mid-size  convertible         NaN   \n",
      "2  full-size        truck        grey   \n",
      "3  full-size        sedan      silver   \n",
      "4        NaN        sedan         NaN   \n",
      "\n",
      "                                           image_url  \\\n",
      "0  https://images.craigslist.org/00b0b_2OOpaB40is...   \n",
      "1  https://images.craigslist.org/00o0o_giVSk7Lu1w...   \n",
      "2  https://images.craigslist.org/00v0v_1w1Oy7wYyl...   \n",
      "3  https://images.craigslist.org/01414_aogQkfToLX...   \n",
      "4  https://images.craigslist.org/00f0f_ezGhBd7oTO...   \n",
      "\n",
      "                                         description state  \\\n",
      "0  I am a private seller, not a dealer. I am list...    nj   \n",
      "1  1967 CHEVROLET CAMARO CONVERTIBLE  ****NUMBERS...    co   \n",
      "2  99 YUKON DENALI - RUNS GREAT! FULLY LOADED DEN...    fl   \n",
      "3  This car is in perfect condition mechanically....    az   \n",
      "4  2017 Lexus ES ES 350 FOR ONLY $398/mo!24,782 m...    ca   \n",
      "\n",
      "                   lat                 long              posting_date  \n",
      "0              41.0128             -74.3338  2020-11-29T11:31:13-0500  \n",
      "1              39.9057            -104.8509  2020-12-03T05:43:21-0700  \n",
      "2              28.2764             -80.6568  2020-11-14T09:18:30-0500  \n",
      "3              32.6168            -114.4947  2020-11-10T13:43:37-0800  \n",
      "4              37.9632            -122.5095  2020-12-01T14:02:14-0800  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# To see what kind of data we're working with\n",
    "print(data_train_ft.dtypes)\n",
    "print(data_train_ft.head(5))\n",
    "\n",
    "# From looking at the data it looks like Draft and all star apperances should be useful. \n",
    "# Feauture used will need to be transformed into numbers in order to use the prediction models and have them be uniform≥ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "Convert numbers to their correct formats and fill in any missing values. \n",
    "\n",
    "Missing values don't help us in our quest for accuracy, but filling them in as zero won't necessarily hurt us either. By filling them in, we can still use other parts of the player's data without having to throw away the entire entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  year  manufacturer_0  manufacturer_1  manufacturer_2  \\\n",
      "0           2,010.0000               0               0               0   \n",
      "1           1,967.0000               0               0               0   \n",
      "2           1,999.0000               0               0               0   \n",
      "3           2,016.0000               0               0               0   \n",
      "4           2,017.0000               0               0               0   \n",
      "5           2,010.0000               0               0               0   \n",
      "6           2,017.0000               0               0               0   \n",
      "7           2,008.0000               0               0               0   \n",
      "8           2,001.0000               0               0               0   \n",
      "9           2,018.0000               0               0               0   \n",
      "\n",
      "   manufacturer_3  manufacturer_4  manufacturer_5  model_0  model_1  model_2  \\\n",
      "0               0               0               1        0        0        0   \n",
      "1               0               1               0        0        0        0   \n",
      "2               0               1               1        0        0        0   \n",
      "3               1               0               0        0        0        0   \n",
      "4               1               0               1        0        0        0   \n",
      "5               1               1               0        0        0        0   \n",
      "6               1               1               1        0        0        0   \n",
      "7               1               1               1        0        0        0   \n",
      "8               1               0               0        0        0        0   \n",
      "9               1               0               0        0        0        0   \n",
      "\n",
      "   ...  paint_color_0  paint_color_1  paint_color_2  paint_color_3  state_0  \\\n",
      "0  ...              0              0              0              1        0   \n",
      "1  ...              0              0              1              0        0   \n",
      "2  ...              0              0              1              1        0   \n",
      "3  ...              0              1              0              0        0   \n",
      "4  ...              0              0              1              0        0   \n",
      "5  ...              0              0              0              1        0   \n",
      "6  ...              0              1              0              1        0   \n",
      "7  ...              0              1              0              0        0   \n",
      "8  ...              0              1              1              0        0   \n",
      "9  ...              0              1              0              1        0   \n",
      "\n",
      "   state_1  state_2  state_3  state_4  state_5  \n",
      "0        0        0        0        0        1  \n",
      "1        0        0        0        1        0  \n",
      "2        0        0        0        1        1  \n",
      "3        0        0        1        0        0  \n",
      "4        0        0        1        0        1  \n",
      "5        0        0        0        0        1  \n",
      "6        0        0        1        0        1  \n",
      "7        0        0        0        0        1  \n",
      "8        0        0        1        1        0  \n",
      "9        0        0        1        1        1  \n",
      "\n",
      "[10 rows x 52 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5 6 8 12 13\n",
    "\n",
    "features = [#'url',\n",
    "           #'region',\n",
    "           #'region_url',\n",
    "           'year',\n",
    "           'manufacturer',\n",
    "           'model',\n",
    "           'condition',\n",
    "           #'cylinders',\n",
    "           'fuel',\n",
    "           'odometer',\n",
    "           #'title_status',\n",
    "           'transmission',\n",
    "           #'VIN',\n",
    "           'drive',\n",
    "           'size',\n",
    "           'type',\n",
    "           'paint_color',\n",
    "           #'image_url',\n",
    "           #'description',\n",
    "           'state',\n",
    "           #'lat',\n",
    "           #'long',\n",
    "          # 'posting_date'\n",
    "          ]\n",
    "data_train_ft['posting_date'] = pd.to_datetime(data_train_ft['posting_date'], utc=True)\n",
    "data_train_ft['posting_date'] = pd.to_datetime(data_train_ft['posting_date'])\n",
    "data_train_ft['posting_date'] = (pd.DatetimeIndex(data_train_ft['posting_date']).day) + (pd.DatetimeIndex(data_train_ft['posting_date']).month*12) + ((2022 - pd.DatetimeIndex(data_train_ft['posting_date']).year)*365)\n",
    "data_train_ft['posting_date'] = data_train_ft['posting_date'].astype(float)\n",
    "#print(data_train_ft['posting_date'].head(5))\n",
    "data_test_df['posting_date'] = pd.to_datetime(data_test_df['posting_date'], utc=True)\n",
    "data_test_df['posting_date'] = pd.to_datetime(data_test_df['posting_date'])\n",
    "data_test_df['posting_date'] = (pd.DatetimeIndex(data_test_df['posting_date']).day) + (pd.DatetimeIndex(data_test_df['posting_date']).month*12) + ((2022 - pd.DatetimeIndex(data_test_df['posting_date']).year)*365)\n",
    "data_test_df['posting_date'] = data_test_df['posting_date'].astype(float)\n",
    "#print(data_test_df['posting_date'].head(5))\n",
    "\n",
    "data_train_ft = data_train_ft[features]\n",
    "data_train_ft = data_train_ft.fillna(0)\n",
    "\n",
    "data_test_ft = data_test_df\n",
    "data_test_df = data_test_df[features]\n",
    "data_test_df = data_test_df.fillna(0)\n",
    "\n",
    "'''\n",
    "TE_encoder = TargetEncoder()\n",
    "data_train_ft = TE_encoder.fit_transform(data_train_ft, data_train_tgt)\n",
    "data_test_df = TE_encoder.transform(data_test_df)\n",
    "'''\n",
    "'''\n",
    "LOOE_encoder = LeaveOneOutEncoder()\n",
    "data_train_ft = LOOE_encoder.fit_transform(data_train_ft, data_train_tgt)\n",
    "data_test_df = LOOE_encoder.transform(data_test_df)\n",
    "'''\n",
    "encoder = ce.BinaryEncoder()\n",
    "data_train_ft = encoder.fit_transform(data_train_ft)\n",
    "data_test_df = encoder.fit_transform(data_test_df)\n",
    "print(data_train_ft.head(10))\n",
    "\n",
    "#LE_encoder = OrdinalEncoder(features)\n",
    "#data_train_ft = LE_encoder.fit_transform(data_train_ft)\n",
    "#data_test_df = LE_encoder.transform(data_test_df)\n",
    "#data_train_ft.head()\n",
    "\n",
    "'''\n",
    "ftrsel = ftr_sel.SelectKBest(ftr_sel.f_classif, k=5)\n",
    "ftrsel.fit_transform(data_train_ft, data_train_tgt)\n",
    "\n",
    "keepers_idx = ftrsel.get_support()\n",
    "print(keepers_idx)\n",
    "# use target encoding to encode two categorical features\n",
    "\n",
    "ftrsel = ftr_sel.SelectFromModel(ensemble.RandomForestClassifier(), \n",
    "                                 threshold='mean') # default\n",
    "'''\n",
    "#ftrsel = ftr_sel.RFE(linear_model.LinearRegression(),\n",
    "                   #  n_features_to_select=5)\n",
    "'''\n",
    "train_plus_validation_ftrs, test_ftrs, train_plus_validation_tgt, test_tgt = train_test_split (data_train_ft, data_train_tgt, test_size = 0.20,\n",
    "                                   random_state = 42)\n",
    "# ordinal encode input variables\n",
    "ordinal_encoder = OrdinalEncoder(features)\n",
    "ordinal_encoder.fit(train_plus_validation_ftrs)\n",
    "train_plus_validation_ftrs = ordinal_encoder.transform(train_plus_validation_ftrs)\n",
    "test_ftrs = ordinal_encoder.transform(test_ftrs)\n",
    "print(train_plus_validation_ftrs.head())\n",
    "\n",
    "test_ftrs = test_ftrs.astype(float)\n",
    "train_plus_validation_ftrs = train_plus_validation_ftrs.astype(float)\n",
    "\n",
    "# ordinal encode target variable\n",
    "encoder = ce.BinaryEncoder()\n",
    "train_plus_validation_tgt = encoder.fit_transform(train_plus_validation_tgt)\n",
    "test_tgt = encoder.fit_transform(test_tgt)\n",
    "print(train_plus_validation_tgt.head())\n",
    "\n",
    "train_plus_validation_tgt = train_plus_validation_tgt.astype(float)\n",
    "test_tgt = test_tgt.astype(float)\n",
    "'''\n",
    "\n",
    "lmlr = linear_model.LogisticRegression\n",
    "ftrsel = ftr_sel.SelectFromModel(lmlr(penalty='l2')) # thesh is \"small\" coeffs\n",
    "#ftrsel = ftr_sel.RFE(ensemble.RandomForestClassifier(),\n",
    "                   #n_features_to_select=7)\n",
    "ftrsel.fit_transform(data_train_ft, data_train_tgt)\n",
    "\n",
    "print(np.array(data_train_ft.columns.value_counts().index)[ftrsel.get_support()])\n",
    "print(data_train_ft.columns.value_counts().count())\n",
    "#ftrsel = ftr_sel.RFE(ensemble.RandomForestClassifier(),\n",
    "                    # n_features_to_select=7)\n",
    "new_features = np.array(data_train_ft.columns.value_counts().index)[ftrsel.get_support()]\n",
    "data_train_ft = data_train_ft[new_features]\n",
    "data_test_df = data_test_df[new_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the updated Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train_ft:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139008</th>\n",
       "      <td>8,995.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150724</th>\n",
       "      <td>34,995.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63979</th>\n",
       "      <td>5,200.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182137</th>\n",
       "      <td>11,900.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73464</th>\n",
       "      <td>12,400.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51249</th>\n",
       "      <td>3,500.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85241</th>\n",
       "      <td>6,950.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194333</th>\n",
       "      <td>46,995.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18596</th>\n",
       "      <td>24,000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173265</th>\n",
       "      <td>9,995.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "{\\centering\n",
       "\\begin{tabular}{lr}\n",
       "\\toprule\n",
       "{} &                price \\\\\n",
       "\\midrule\n",
       "139008 &           8,995.0000 \\\\\n",
       "150724 &          34,995.0000 \\\\\n",
       "63979  &           5,200.0000 \\\\\n",
       "182137 &          11,900.0000 \\\\\n",
       "73464  &          12,400.0000 \\\\\n",
       "51249  &           3,500.0000 \\\\\n",
       "85241  &           6,950.0000 \\\\\n",
       "194333 &          46,995.0000 \\\\\n",
       "18596  &          24,000.0000 \\\\\n",
       "173265 &           9,995.0000 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\n",
       "\\medskip}"
      ],
      "text/plain": [
       "                      price\n",
       "139008           8,995.0000\n",
       "150724          34,995.0000\n",
       "63979            5,200.0000\n",
       "182137          11,900.0000\n",
       "73464           12,400.0000\n",
       "51249            3,500.0000\n",
       "85241            6,950.0000\n",
       "194333          46,995.0000\n",
       "18596           24,000.0000\n",
       "173265           9,995.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train_ft.info():\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 160732 entries, 139008 to 121958\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   price   160732 non-null  float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 2.5 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train_ft:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139008</th>\n",
       "      <td>8,995.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150724</th>\n",
       "      <td>34,995.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63979</th>\n",
       "      <td>5,200.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182137</th>\n",
       "      <td>11,900.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73464</th>\n",
       "      <td>12,400.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51249</th>\n",
       "      <td>3,500.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85241</th>\n",
       "      <td>6,950.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194333</th>\n",
       "      <td>46,995.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18596</th>\n",
       "      <td>24,000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173265</th>\n",
       "      <td>9,995.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "{\\centering\n",
       "\\begin{tabular}{lr}\n",
       "\\toprule\n",
       "{} &                price \\\\\n",
       "\\midrule\n",
       "139008 &           8,995.0000 \\\\\n",
       "150724 &          34,995.0000 \\\\\n",
       "63979  &           5,200.0000 \\\\\n",
       "182137 &          11,900.0000 \\\\\n",
       "73464  &          12,400.0000 \\\\\n",
       "51249  &           3,500.0000 \\\\\n",
       "85241  &           6,950.0000 \\\\\n",
       "194333 &          46,995.0000 \\\\\n",
       "18596  &          24,000.0000 \\\\\n",
       "173265 &           9,995.0000 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\n",
       "\\medskip}"
      ],
      "text/plain": [
       "                      price\n",
       "139008           8,995.0000\n",
       "150724          34,995.0000\n",
       "63979            5,200.0000\n",
       "182137          11,900.0000\n",
       "73464           12,400.0000\n",
       "51249            3,500.0000\n",
       "85241            6,950.0000\n",
       "194333          46,995.0000\n",
       "18596           24,000.0000\n",
       "173265           9,995.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train_ft.info():\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 160732 entries, 139008 to 121958\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   price   160732 non-null  float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 2.5 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(\"data_train_ft:\")\n",
    "display(data_train_ft.head(10))\n",
    "print(\"data_train_ft.info():\")\n",
    "display(data_train_ft.info())\n",
    "\n",
    "data_train_ft = data_train_ft.astype(float)\n",
    "data_test_df = data_test_df.astype(float)\n",
    "\n",
    "print(\"data_train_ft:\")\n",
    "display(data_train_ft.head(10))\n",
    "print(\"data_train_ft.info():\")\n",
    "display(data_train_ft.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "Create a heat map to visualize trends present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8k/l65lktds3250dyd3jzxfvjw80000gn/T/ipykernel_90321/119115702.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcorrelation_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrcoef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfont_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mheatmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrelation_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.2f'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_features' is not defined"
     ]
    }
   ],
   "source": [
    "correlation_map = np.corrcoef(data_train_ft.values.T)\n",
    "sns.set(font_scale=1.0)\n",
    "heatmap = sns.heatmap(correlation_map, cbar=True, annot=True, square=True, fmt='.2f', yticklabels=features, xticklabels=features)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "Split the data into training and validation sets for use with cross-validation scoring. We split the data randomly to lower the chances of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plus_validation_ftrs, test_ftrs, train_plus_validation_tgt, test_tgt = train_test_split (data_train_ft, data_train_tgt, test_size = 0.20,\n",
    "                                   random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7\n",
    "This is another portion where accuracy is most directly influenced. We added as many models as possible (within reason) to ensure that we were covering all our bases to get the most accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LR': LinearRegression(), 'EL Net': ElasticNet(random_state=0), 'RandomForest-50': RandomForestRegressor(n_estimators=50), 'RandomForest-80': RandomForestRegressor(n_estimators=80), 'RandomForest-110': RandomForestRegressor(n_estimators=110), 'Lasso (C=40)': Lasso(alpha=40), 'Ridge (C=40)': Ridge(alpha=40), 'Lasso (C=50)': Lasso(alpha=50), 'Ridge (C=50)': Ridge(alpha=50), 'Lasso (C=100)': Lasso(alpha=100), 'Ridge (C=100)': Ridge(alpha=100)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipelines = {\n",
    "    'LR' : LinearRegression(),\n",
    "    'EL Net' : ElasticNet(random_state=0),\n",
    "    #'Multi-Net' : linear_model.MultiTaskElasticNet(alpha=0.1)\n",
    "    #'CART' : DecisionTreeRegressor(max_depth=10)\n",
    "    }\n",
    "for k in range(50,120,30):\n",
    "    pipelines[f'RandomForest-{k}'] = sklearn.ensemble.RandomForestRegressor(n_estimators=k)\n",
    "#for k in range(1,11,2):\n",
    "    #pipelines[f'KNN-{k}'] = KNeighborsRegressor(n_neighbors=k)\n",
    "#for i in range(25,45,5):\n",
    "    #pipelines[f'GBR-{i}'] = GradientBoostingRegressor(random_state=21, n_estimators=i)\n",
    "values = [40, 50, 100]\n",
    "for alpha_value in values:\n",
    "    pipelines[f'Lasso (C={alpha_value})'] = linear_model.Lasso(alpha=alpha_value)\n",
    "    pipelines[f'Ridge (C={alpha_value})'] = linear_model.Ridge(alpha=alpha_value)\n",
    "print(pipelines)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8\n",
    "Using different standardizing pipelines, run multiple cross-validation tests on the data splits to find the best performing model.\n",
    "\n",
    "Further refine the selection of best model by comparing scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently testing model: LR\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [160732, 200916]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8k/l65lktds3250dyd3jzxfvjw80000gn/T/ipykernel_90321/3436371192.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#loo = skms.LeaveOneOut()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Currently testing model: {pipeline_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     scores = skms.cross_val_score(pipelines[pipeline_name],\n\u001b[0m\u001b[1;32m      6\u001b[0m                                   \u001b[0mdata_train_ft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                   \u001b[0mdata_train_tgt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n\u001b[0m\u001b[1;32m    446\u001b[0m                                 \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \"\"\"\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \"\"\"\n\u001b[1;32m    355\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    320\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [160732, 200916]"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy_scores = {}\n",
    "for pipeline_name in pipelines:\n",
    "    #loo = skms.LeaveOneOut()\n",
    "    print(f'Currently testing model: {pipeline_name}')\n",
    "    scores = skms.cross_val_score(pipelines[pipeline_name],\n",
    "                                  train_plus_validation_ftrs,\n",
    "                                  train_plus_validation_tgt,\n",
    "                                  #cv=loo,\n",
    "                                  cv=10,\n",
    "                                  scoring='neg_mean_squared_error')\n",
    "    scores = np.sqrt(-scores.mean())\n",
    "    accuracy_scores[pipeline_name] = scores\n",
    "    print(f'{pipeline_name}: {scores:.3f}')\n",
    "\n",
    "best_model_name = min(accuracy_scores,key=accuracy_scores.get)\n",
    "print(f'\\nBest model: {best_model_name} (accuracy = {accuracy_scores[best_model_name]:.3f})')\n",
    "final_model = pipelines[best_model_name]\n",
    "\n",
    "## Took my computer 3 min 14 sec to run this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9\n",
    "Rescale the data and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(train_plus_validation_ftrs)\n",
    "rescaled_train_plus_validation_ftrs = scaler.transform(train_plus_validation_ftrs)\n",
    "car_scores_fit = final_model.fit(rescaled_train_plus_validation_ftrs, \n",
    "                                   train_plus_validation_tgt)\n",
    "rescaled_test_ftrs = scaler.transform(test_ftrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10\n",
    "Scale the data and predict points on the selected model and save it in a temporary .csv for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: ElasticNet(random_state=0) and features: ['fuel' 'title_status' 'drive' 'size' 'type']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = car_scores_fit.predict(rescaled_test_ftrs)\n",
    "# score = np.sqrt(metrics.mean_squared_error(predictions, tes))\n",
    "print(f'Using model: {final_model} and features: {new_features}')\n",
    "# print(f'Final score: {score:.3f}')\n",
    "# predictions = model.predict(rescaled_test_ftrs)\n",
    "\n",
    "# compare = pd.DataFrame({'Prediction': predictions, 'Test Data' : test_tgt})\n",
    "# compare.head(10)\n",
    "# This is just using the test.csv to setup a dataframe of the correct size\n",
    "# and indicies (the \"id\" field).\n",
    "make_submission_df = pd.read_csv(\"cars_reg_test.csv\")\n",
    "# drop all columns except 'id'\n",
    "make_submission_df = make_submission_df[['id']]\n",
    "# make sure the column of ID's that we just read in is the index column\n",
    "make_submission_df = make_submission_df.set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11\n",
    "Add our predictions to a submission file and save the final .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission_df['price'] = predictions\n",
    "make_submission_df.to_csv('submission.csv',sep=',', float_format='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
