{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used Car Regression\n",
    "**ML Project 2 - Spring 2022**\n",
    "\n",
    "Collaborators: Ben DeSollar and Matt McDonell\n",
    "\n",
    "Through research we found that a called neural network model called KerasRegressor worked the best after testing a couple different model including\n",
    "    KNeighborsRegressor, Lasso, Ridge, DecisionTreeRegressor, and RandomForestRegressor. We improved our results by modifying our feature selection to include as many features as we \n",
    "    could to help predict the best values. By modifying our featuiues \n",
    "    during testing we were able to find the best model predictions, which turned out to be all feautures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "Import the required packages and frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from mlwpy import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import (datasets, neighbors,\n",
    "                     naive_bayes,\n",
    "                     model_selection as skms,\n",
    "                     linear_model, dummy,\n",
    "                     metrics,\n",
    "                     pipeline,\n",
    "                     preprocessing as skpre) \n",
    "import csv\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import category_encoders as ce\n",
    "import pandas.util.testing as tm\n",
    "import category_encoders as ce\n",
    "\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(1)\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Read in the .csv files and create DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_df = pd.read_csv(\"cars_reg_train.csv\") \n",
    "data_test_df = pd.read_csv(\"cars_reg_test.csv\")\n",
    "data_train_ft = data_train_df.drop('price', axis=1)\n",
    "data_train_tgt = data_train_df[\"price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "Focus down to the features we think will have the largest on the price\n",
    "\n",
    "We originally looked at features that we thought would influence the price of the car the most, such as year, fuel, odemeter, and type. However this assumption was proved incorrect after doing feature engineering later on and finding that using all of the features produced the most accurate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see what kind of data we're working with\n",
    "print(data_train_ft.dtypes)\n",
    "print(data_train_ft.head(5))\n",
    "\n",
    "# From looking at the data it looks like Draft and all star apperances should be useful. \n",
    "# Feauture used will need to be transformed into numbers in order to use the prediction models and have them be uniformâ‰¥ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "Convert numbers to their correct formats and fill in any missing values. \n",
    "\n",
    "Missing values don't help us in our quest for accuracy, but filling them in as zero won't necessarily hurt us either.\n",
    "\n",
    "Also concert all categorical values using a target encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['url',\n",
    "           'region',\n",
    "           'region_url',\n",
    "           'year',\n",
    "           'manufacturer',\n",
    "           'model',\n",
    "           'condition',\n",
    "           'cylinders',\n",
    "           'fuel',\n",
    "           'odometer',\n",
    "           'title_status',\n",
    "           'transmission',\n",
    "           'VIN',\n",
    "           'drive',\n",
    "           'size',\n",
    "           'type',\n",
    "           'paint_color',\n",
    "           'image_url',\n",
    "           'description',\n",
    "           'state',\n",
    "           'lat',\n",
    "           'long',\n",
    "           'posting_date'\n",
    "          ]\n",
    "data_train_ft['posting_date'] = pd.to_datetime(data_train_ft['posting_date'], utc=True)\n",
    "data_train_ft['posting_date'] = pd.to_datetime(data_train_ft['posting_date'])\n",
    "data_train_ft['posting_date'] = (pd.DatetimeIndex(data_train_ft['posting_date']).day) + (pd.DatetimeIndex(data_train_ft['posting_date']).month*12) + ((2022 - pd.DatetimeIndex(data_train_ft['posting_date']).year)*365)\n",
    "data_train_ft['posting_date'] = data_train_ft['posting_date'].astype(float)\n",
    "print(data_train_ft['posting_date'].head(5))\n",
    "data_test_df['posting_date'] = pd.to_datetime(data_test_df['posting_date'], utc=True)\n",
    "data_test_df['posting_date'] = pd.to_datetime(data_test_df['posting_date'])\n",
    "data_test_df['posting_date'] = (pd.DatetimeIndex(data_test_df['posting_date']).day) + (pd.DatetimeIndex(data_test_df['posting_date']).month*12) + ((2022 - pd.DatetimeIndex(data_test_df['posting_date']).year)*365)\n",
    "data_test_df['posting_date'] = data_test_df['posting_date'].astype(float)\n",
    "print(data_test_df['posting_date'].head(5))\n",
    "\n",
    "data_train_ft = data_train_ft[features]\n",
    "data_train_ft = data_train_ft.fillna(0)\n",
    "\n",
    "data_test_ft = data_test_df\n",
    "data_test_df = data_test_df[features]\n",
    "data_test_df = data_test_df.fillna(0)\n",
    "\n",
    "encoder = ce.TargetEncoder()\n",
    "data_train_ft = encoder.fit_transform(data_train_ft, data_train_tgt)\n",
    "data_test_df = encoder.transform(data_test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "Pick the best catgeory by using feature engineering. Found that using all features pruduced the best results. We \n",
    "originally assumed that the urls would hurt the model, but found this to not be the case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmlr = linear_model.LogisticRegression\n",
    "\n",
    "ftrsel = ftr_sel.RFE(linear_model.LinearRegression(), n_features_to_select=23)\n",
    "\n",
    "ftrsel.fit_transform(data_train_ft, data_train_tgt)\n",
    "\n",
    "print(np.array(data_train_ft.columns.value_counts().index)[ftrsel.get_support()])\n",
    "print(data_train_ft.columns.value_counts().count())\n",
    "new_features = np.array(data_train_ft.columns.value_counts().index)[ftrsel.get_support()]\n",
    "data_train_ft = data_train_ft[new_features]\n",
    "data_test_df = data_test_df[new_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "Show the updated Dataframes and then convert all datatypes to type float and reshow dataframes with new info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"data_train_ft:\")\n",
    "display(data_train_ft.head(10))\n",
    "print(\"data_train_ft.info():\")\n",
    "display(data_train_ft.info())\n",
    "\n",
    "data_train_ft = data_train_ft.astype(float)\n",
    "data_test_df = data_test_df.astype(float)\n",
    "\n",
    "print(\"data_train_ft:\")\n",
    "display(data_train_ft.head(10))\n",
    "print(\"data_train_ft.info():\")\n",
    "display(data_train_ft.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7\n",
    "Create a heat map to visualize trends present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_map = np.corrcoef(data_train_ft.values.T)\n",
    "sns.set(font_scale=1.0)\n",
    "heatmap = sns.heatmap(correlation_map, cbar=True, annot=True, square=True, fmt='.2f', yticklabels=new_features, xticklabels=new_features)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8\n",
    "Scale the data to prevent outliers from affecting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def scale_datasets(x_train, x_test):\n",
    "\n",
    "  \"\"\"\n",
    "  Standard Scale test and train data\n",
    "  \"\"\"\n",
    "  standard_scaler = MinMaxScaler()\n",
    "  x_train_scaled = pd.DataFrame(\n",
    "      standard_scaler.fit_transform(x_train),\n",
    "      columns=x_train.columns\n",
    "  )\n",
    "  x_test_scaled = pd.DataFrame(\n",
    "      standard_scaler.transform(x_test),\n",
    "      columns = x_test.columns\n",
    "  )\n",
    "  return x_train_scaled, x_test_scaled\n",
    "\n",
    "data_train_ft, data_test_df = scale_datasets(data_train_ft, data_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9\n",
    "This is another portion where accuracy is most directly influenced. We added as many models as possible (within reason) to ensure that we were covering all our bases to get the most accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipelines = {\n",
    "    'LR' : LinearRegression(),\n",
    "    'EL Net' : ElasticNet(random_state=0),\n",
    "    #'CART' : DecisionTreeRegressor(max_depth=10)\n",
    "    'RandomForest' : sklearn.ensemble.RandomForestRegressor()\n",
    "    }\n",
    "# Commented out models did not perform well enough to keep testing\n",
    "#for k in range(1,11,2):\n",
    "    #pipelines[f'KNN-{k}'] = KNeighborsRegressor(n_neighbors=k)\n",
    "#for i in range(25,45,5):\n",
    "    #pipelines[f'GBR-{i}'] = GradientBoostingRegressor(random_state=21, n_estimators=i)\n",
    "values = [40, 50, 100]\n",
    "for alpha_value in values:\n",
    "    pipelines[f'Lasso (C={alpha_value})'] = linear_model.Lasso(alpha=alpha_value)\n",
    "    pipelines[f'Ridge (C={alpha_value})'] = linear_model.Ridge(alpha=alpha_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10\n",
    "Using different standardizing pipelines, run multiple cross-validation tests on the data splits to find the best performing model.\n",
    "\n",
    "Further refine the selection of best model by comparing scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores = {}\n",
    "for pipeline_name in pipelines:\n",
    "    #loo = skms.LeaveOneOut()\n",
    "    print(f'Currently testing model: {pipeline_name}')\n",
    "    scores = skms.cross_val_score(pipelines[pipeline_name],\n",
    "                                  data_train_ft,\n",
    "                                  data_train_tgt,\n",
    "                                  #cv=loo,\n",
    "                                  cv=10,\n",
    "                                  scoring='neg_mean_squared_error')\n",
    "    scores = np.sqrt(-scores.mean())\n",
    "    accuracy_scores[pipeline_name] = scores\n",
    "    print(f'{pipeline_name}: {scores:.3f}')\n",
    "\n",
    "best_model_name = min(accuracy_scores,key=accuracy_scores.get)\n",
    "print(f'\\nBest model: {best_model_name} (accuracy = {accuracy_scores[best_model_name]:.3f})')\n",
    "final_model = pipelines[best_model_name]\n",
    "\n",
    "car_scores_fit = final_model.fit(data_train_ft, \n",
    "                                   data_train_tgt)\n",
    "predictions_normal = car_scores_fit.predict(data_test_df)\n",
    "\n",
    "    \n",
    "acuuracy = np.sqrt(mean_squared_error(data_train_tgt.iloc[0:predictions_normal.shape[0]],predictions_normal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11\n",
    "Now we tested using a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units1 = 160\n",
    "hidden_units2 = 480\n",
    "hidden_units3 = 256\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Creating model using the Sequential in tensorflow\n",
    "def build_model_using_sequential():\n",
    "  model = Sequential([\n",
    "    Dense(hidden_units1, kernel_initializer='normal', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(hidden_units2, kernel_initializer='normal', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(hidden_units3, kernel_initializer='normal', activation='relu'),\n",
    "    Dense(1, kernel_initializer='normal', activation='linear')\n",
    "  ])\n",
    "  return model\n",
    "# build the model\n",
    "model = build_model_using_sequential()\n",
    "\n",
    "# loss function\n",
    "msle = MeanSquaredError()\n",
    "model.compile(\n",
    "    loss='mean_squared_error', \n",
    "    #optimizer=Adam(learning_rate=0.01), \n",
    "    metrics=[msle]\n",
    ")\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    data_train_ft, \n",
    "    data_train_tgt, \n",
    "    epochs=5, \n",
    "    batch_size=64,\n",
    "    validation_split=0.2\n",
    ")\n",
    "predictions_nn = model.predict(data_test_df)\n",
    "acuuracy_nn = np.sqrt(mean_squared_error(data_train_tgt.iloc[0:predictions_nn.shape[0]],predictions_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12\n",
    "Select the best model based on the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if acuuracy_nn < acuuracy:\n",
    "    predictions = predictions_nn\n",
    "else:\n",
    "    predictions = predictions_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13\n",
    "Add our predictions to a submission file and save the final .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just using the test.csv to setup a dataframe of the correct size\n",
    "# and indicies (the \"id\" field).\n",
    "make_submission_df = pd.read_csv(\"cars_reg_test.csv\")\n",
    "# drop all columns except 'id'\n",
    "make_submission_df = make_submission_df[['id']]\n",
    "# make sure the column of ID's that we just read in is the index column\n",
    "make_submission_df = make_submission_df.set_index('id')\n",
    "make_submission_df['price'] = predictions\n",
    "make_submission_df.to_csv('submission.csv',sep=',', float_format='%.2f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
